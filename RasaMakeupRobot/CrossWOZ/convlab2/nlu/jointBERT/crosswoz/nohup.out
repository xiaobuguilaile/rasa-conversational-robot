train set size: 84692

max sen bert len 126
[(1, 1), (2, 1), (3, 24), (4, 268), (5, 137), (6, 158), (7, 157), (8, 187), (9, 188), (10, 312), (11, 245), (12, 339), (13, 295), (14, 334), (15, 261), (16, 237), (17, 261), (18, 245), (19, 207), (20, 207), (21, 190), (22, 200), (23, 207), (24, 163), (25, 165), (26, 163), (27, 174), (28, 156), (29, 157), (30, 155), (31, 173), (32, 153), (33, 168), (34, 124), (35, 121), (36, 131), (37, 128), (38, 125), (39, 111), (40, 101), (41, 114), (42, 93), (43, 90), (44, 100), (45, 83), (46, 57), (47, 72), (48, 75), (49, 61), (50, 46), (51, 38), (52, 48), (53, 43), (54, 31), (55, 30), (56, 35), (57, 30), (58, 26), (59, 25), (60, 22), (61, 16), (62, 19), (63, 17), (64, 13), (65, 10), (66, 20), (67, 16), (68, 9), (69, 14), (70, 2), (71, 8), (72, 4), (73, 5), (74, 7), (75, 6), (76, 7), (77, 3), (78, 4), (79, 3), (80, 1), (82, 2), (83, 3), (84, 5), (85, 2), (87, 1), (89, 1), (90, 1), (92, 1), (93, 2), (96, 2), (100, 1), (101, 1), (103, 1), (109, 1), (112, 1), (126, 1)]

max context bert len 209
[(3, 500), (14, 1), (17, 1), (19, 1), (20, 3), (21, 6), (22, 4), (23, 5), (24, 6), (25, 11), (26, 10), (27, 17), (28, 16), (29, 17), (30, 29), (31, 27), (32, 36), (33, 50), (34, 41), (35, 53), (36, 61), (37, 66), (38, 64), (39, 63), (40, 65), (41, 63), (42, 74), (43, 63), (44, 69), (45, 89), (46, 82), (47, 79), (48, 78), (49, 79), (50, 81), (51, 74), (52, 91), (53, 73), (54, 92), (55, 83), (56, 70), (57, 103), (58, 93), (59, 108), (60, 81), (61, 114), (62, 120), (63, 102), (64, 102), (65, 117), (66, 87), (67, 118), (68, 102), (69, 117), (70, 126), (71, 91), (72, 103), (73, 98), (74, 104), (75, 105), (76, 100), (77, 112), (78, 105), (79, 110), (80, 106), (81, 93), (82, 92), (83, 100), (84, 99), (85, 101), (86, 97), (87, 104), (88, 90), (89, 86), (90, 79), (91, 83), (92, 88), (93, 84), (94, 86), (95, 72), (96, 84), (97, 93), (98, 65), (99, 92), (100, 57), (101, 68), (102, 62), (103, 69), (104, 65), (105, 70), (106, 45), (107, 59), (108, 51), (109, 42), (110, 51), (111, 57), (112, 37), (113, 47), (114, 50), (115, 46), (116, 52), (117, 46), (118, 44), (119, 32), (120, 26), (121, 26), (122, 30), (123, 36), (124, 33), (125, 29), (126, 29), (127, 24), (128, 21), (129, 29), (130, 29), (131, 27), (132, 24), (133, 22), (134, 19), (135, 18), (136, 15), (137, 21), (138, 18), (139, 11), (140, 14), (141, 14), (142, 14), (143, 17), (144, 11), (145, 9), (146, 17), (147, 16), (148, 13), (149, 8), (150, 18), (151, 7), (152, 7), (153, 13), (154, 11), (155, 3), (156, 7), (157, 6), (158, 3), (159, 3), (160, 6), (161, 3), (162, 7), (163, 4), (164, 4), (165, 6), (166, 3), (167, 5), (168, 7), (169, 4), (170, 6), (171, 3), (172, 5), (173, 2), (174, 2), (175, 3), (176, 1), (178, 2), (179, 2), (180, 5), (182, 2), (183, 3), (184, 3), (185, 2), (186, 1), (189, 2), (190, 1), (191, 1), (193, 1), (194, 1), (195, 1), (197, 1), (201, 3), (206, 1), (208, 1), (209, 1)]
val set size: 8458

max sen bert len 199
[(2, 2), (3, 25), (4, 302), (5, 123), (6, 152), (7, 138), (8, 259), (9, 179), (10, 291), (11, 215), (12, 344), (13, 266), (14, 323), (15, 289), (16, 236), (17, 247), (18, 233), (19, 209), (20, 224), (21, 184), (22, 184), (23, 192), (24, 176), (25, 209), (26, 194), (27, 132), (28, 190), (29, 160), (30, 181), (31, 140), (32, 162), (33, 137), (34, 119), (35, 121), (36, 117), (37, 134), (38, 122), (39, 131), (40, 112), (41, 107), (42, 106), (43, 95), (44, 110), (45, 73), (46, 53), (47, 55), (48, 70), (49, 63), (50, 57), (51, 50), (52, 49), (53, 37), (54, 55), (55, 35), (56, 32), (57, 31), (58, 27), (59, 23), (60, 20), (61, 18), (62, 12), (63, 13), (64, 12), (65, 9), (66, 11), (67, 6), (68, 8), (69, 6), (70, 7), (71, 3), (72, 5), (73, 6), (74, 7), (75, 2), (76, 7), (77, 2), (78, 3), (79, 4), (80, 5), (81, 3), (82, 3), (83, 5), (84, 2), (85, 1), (88, 2), (89, 1), (90, 1), (91, 1), (94, 1), (98, 1), (100, 1), (106, 1), (111, 1), (113, 1), (114, 1), (119, 1), (199, 1)]

max context bert len 275
[(3, 500), (15, 1), (18, 3), (19, 1), (20, 4), (21, 6), (22, 6), (23, 3), (24, 11), (25, 13), (26, 19), (27, 10), (28, 19), (29, 12), (30, 18), (31, 36), (32, 29), (33, 43), (34, 42), (35, 45), (36, 52), (37, 55), (38, 39), (39, 50), (40, 79), (41, 77), (42, 60), (43, 76), (44, 77), (45, 74), (46, 84), (47, 100), (48, 78), (49, 87), (50, 92), (51, 106), (52, 86), (53, 80), (54, 94), (55, 98), (56, 85), (57, 95), (58, 94), (59, 89), (60, 96), (61, 107), (62, 82), (63, 111), (64, 111), (65, 92), (66, 104), (67, 99), (68, 114), (69, 127), (70, 92), (71, 105), (72, 102), (73, 124), (74, 107), (75, 98), (76, 117), (77, 113), (78, 116), (79, 93), (80, 97), (81, 96), (82, 107), (83, 98), (84, 93), (85, 107), (86, 91), (87, 102), (88, 95), (89, 78), (90, 103), (91, 94), (92, 92), (93, 81), (94, 81), (95, 89), (96, 85), (97, 67), (98, 87), (99, 63), (100, 71), (101, 61), (102, 60), (103, 60), (104, 63), (105, 71), (106, 69), (107, 51), (108, 61), (109, 56), (110, 44), (111, 53), (112, 50), (113, 47), (114, 49), (115, 44), (116, 42), (117, 42), (118, 41), (119, 43), (120, 32), (121, 33), (122, 32), (123, 25), (124, 30), (125, 23), (126, 28), (127, 20), (128, 20), (129, 25), (130, 13), (131, 30), (132, 20), (133, 33), (134, 28), (135, 14), (136, 15), (137, 13), (138, 12), (139, 13), (140, 9), (141, 17), (142, 11), (143, 12), (144, 11), (145, 15), (146, 11), (147, 11), (148, 11), (149, 12), (150, 10), (151, 10), (152, 8), (153, 5), (154, 10), (155, 7), (156, 5), (157, 2), (158, 6), (159, 10), (160, 3), (161, 4), (162, 4), (163, 3), (164, 6), (165, 1), (166, 3), (167, 8), (168, 2), (169, 4), (170, 4), (171, 5), (172, 2), (173, 2), (174, 1), (175, 4), (176, 2), (177, 2), (179, 2), (180, 4), (181, 1), (182, 2), (183, 3), (184, 2), (185, 4), (186, 1), (187, 2), (189, 2), (192, 2), (194, 1), (196, 1), (197, 1), (200, 3), (202, 2), (204, 1), (208, 1), (211, 1), (212, 2), (231, 1), (249, 1), (252, 1), (275, 1)]

test set size: 8476
hfl/chinese-bert-wwm-ext
Downloading: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 647/647 [00:00<00:00, 258kB/s]
Downloading: 100%|██████████████████████████████████████████████████████████████████████████████████████████████| 412M/412M [00:53<00:00, 7.71MB/s]
bert.embeddings.word_embeddings.weight torch.Size([21128, 768]) cuda:0 True
bert.embeddings.position_embeddings.weight torch.Size([512, 768]) cuda:0 True
bert.embeddings.token_type_embeddings.weight torch.Size([2, 768]) cuda:0 True
bert.embeddings.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.embeddings.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.0.attention.self.query.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.0.attention.self.query.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.0.attention.self.key.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.0.attention.self.key.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.0.attention.self.value.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.0.attention.self.value.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.0.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.0.attention.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.0.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.0.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.0.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True
bert.encoder.layer.0.intermediate.dense.bias torch.Size([3072]) cuda:0 True
bert.encoder.layer.0.output.dense.weight torch.Size([768, 3072]) cuda:0 True
bert.encoder.layer.0.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.0.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.0.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.1.attention.self.query.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.1.attention.self.query.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.1.attention.self.key.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.1.attention.self.key.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.1.attention.self.value.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.1.attention.self.value.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.1.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.1.attention.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.1.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.1.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.1.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True
bert.encoder.layer.1.intermediate.dense.bias torch.Size([3072]) cuda:0 True
bert.encoder.layer.1.output.dense.weight torch.Size([768, 3072]) cuda:0 True
bert.encoder.layer.1.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.1.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.1.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.2.attention.self.query.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.2.attention.self.query.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.2.attention.self.key.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.2.attention.self.key.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.2.attention.self.value.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.2.attention.self.value.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.2.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.2.attention.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.2.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.2.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.2.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True
bert.encoder.layer.2.intermediate.dense.bias torch.Size([3072]) cuda:0 True
bert.encoder.layer.2.output.dense.weight torch.Size([768, 3072]) cuda:0 True
bert.encoder.layer.2.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.2.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.2.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.3.attention.self.query.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.3.attention.self.query.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.3.attention.self.key.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.3.attention.self.key.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.3.attention.self.value.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.3.attention.self.value.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.3.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.3.attention.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.3.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.3.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.3.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True
bert.encoder.layer.3.intermediate.dense.bias torch.Size([3072]) cuda:0 True
bert.encoder.layer.3.output.dense.weight torch.Size([768, 3072]) cuda:0 True
bert.encoder.layer.3.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.3.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.3.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.4.attention.self.query.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.4.attention.self.query.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.4.attention.self.key.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.4.attention.self.key.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.4.attention.self.value.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.4.attention.self.value.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.4.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.4.attention.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.4.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.4.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.4.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True
bert.encoder.layer.4.intermediate.dense.bias torch.Size([3072]) cuda:0 True
bert.encoder.layer.4.output.dense.weight torch.Size([768, 3072]) cuda:0 True
bert.encoder.layer.4.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.4.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.4.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.5.attention.self.query.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.5.attention.self.query.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.5.attention.self.key.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.5.attention.self.key.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.5.attention.self.value.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.5.attention.self.value.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.5.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.5.attention.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.5.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.5.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.5.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True
bert.encoder.layer.5.intermediate.dense.bias torch.Size([3072]) cuda:0 True
bert.encoder.layer.5.output.dense.weight torch.Size([768, 3072]) cuda:0 True
bert.encoder.layer.5.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.5.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.5.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.6.attention.self.query.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.6.attention.self.query.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.6.attention.self.key.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.6.attention.self.key.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.6.attention.self.value.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.6.attention.self.value.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.6.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.6.attention.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.6.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.6.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.6.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True
bert.encoder.layer.6.intermediate.dense.bias torch.Size([3072]) cuda:0 True
bert.encoder.layer.6.output.dense.weight torch.Size([768, 3072]) cuda:0 True
bert.encoder.layer.6.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.6.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.6.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.7.attention.self.query.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.7.attention.self.query.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.7.attention.self.key.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.7.attention.self.key.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.7.attention.self.value.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.7.attention.self.value.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.7.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.7.attention.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.7.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.7.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.7.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True
bert.encoder.layer.7.intermediate.dense.bias torch.Size([3072]) cuda:0 True
bert.encoder.layer.7.output.dense.weight torch.Size([768, 3072]) cuda:0 True
bert.encoder.layer.7.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.7.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.7.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.8.attention.self.query.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.8.attention.self.query.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.8.attention.self.key.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.8.attention.self.key.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.8.attention.self.value.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.8.attention.self.value.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.8.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.8.attention.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.8.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.8.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.8.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True
bert.encoder.layer.8.intermediate.dense.bias torch.Size([3072]) cuda:0 True
bert.encoder.layer.8.output.dense.weight torch.Size([768, 3072]) cuda:0 True
bert.encoder.layer.8.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.8.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.8.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.9.attention.self.query.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.9.attention.self.query.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.9.attention.self.key.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.9.attention.self.key.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.9.attention.self.value.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.9.attention.self.value.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.9.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.9.attention.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.9.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.9.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.9.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True
bert.encoder.layer.9.intermediate.dense.bias torch.Size([3072]) cuda:0 True
bert.encoder.layer.9.output.dense.weight torch.Size([768, 3072]) cuda:0 True
bert.encoder.layer.9.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.9.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.9.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.10.attention.self.query.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.10.attention.self.query.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.10.attention.self.key.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.10.attention.self.key.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.10.attention.self.value.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.10.attention.self.value.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.10.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.10.attention.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.10.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.10.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.10.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True
bert.encoder.layer.10.intermediate.dense.bias torch.Size([3072]) cuda:0 True
bert.encoder.layer.10.output.dense.weight torch.Size([768, 3072]) cuda:0 True
bert.encoder.layer.10.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.10.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.10.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.11.attention.self.query.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.11.attention.self.query.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.11.attention.self.key.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.11.attention.self.key.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.11.attention.self.value.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.11.attention.self.value.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.11.attention.output.dense.weight torch.Size([768, 768]) cuda:0 True
bert.encoder.layer.11.attention.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.11.attention.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.11.attention.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.11.intermediate.dense.weight torch.Size([3072, 768]) cuda:0 True
bert.encoder.layer.11.intermediate.dense.bias torch.Size([3072]) cuda:0 True
bert.encoder.layer.11.output.dense.weight torch.Size([768, 3072]) cuda:0 True
bert.encoder.layer.11.output.dense.bias torch.Size([768]) cuda:0 True
bert.encoder.layer.11.output.LayerNorm.weight torch.Size([768]) cuda:0 True
bert.encoder.layer.11.output.LayerNorm.bias torch.Size([768]) cuda:0 True
bert.pooler.dense.weight torch.Size([768, 768]) cuda:0 True
bert.pooler.dense.bias torch.Size([768]) cuda:0 True
intent_classifier.weight torch.Size([158, 768]) cuda:0 True
intent_classifier.bias torch.Size([158]) cuda:0 True
slot_classifier.weight torch.Size([77, 768]) cuda:0 True
slot_classifier.bias torch.Size([77]) cuda:0 True
intent_hidden.weight torch.Size([768, 768]) cuda:0 True
intent_hidden.bias torch.Size([768]) cuda:0 True
slot_hidden.weight torch.Size([768, 768]) cuda:0 True
slot_hidden.bias torch.Size([768]) cuda:0 True
[1000|40000] step
	 slot loss: 0.3169262554394081
	 intent loss: 0.06468510022200644
8458 samples val
	 slot loss: 0.17752858766588717
	 intent loss: 0.021619576704462642
--------------------intent--------------------
	 Precision: 82.32
	 Recall: 43.39
	 F1: 56.83
--------------------slot--------------------
	 Precision: 76.68
	 Recall: 73.34
	 F1: 74.97
--------------------overall--------------------
	 Precision: 78.11
	 Recall: 61.91
	 F1: 69.07
best val F1 0.6907
save on crosswoz/output/all
[2000|40000] step
	 slot loss: 0.1605157857472077
	 intent loss: 0.017750665794010273
8458 samples val
	 slot loss: 0.1523476229121939
	 intent loss: 0.012185728471205573
--------------------intent--------------------
	 Precision: 86.40
	 Recall: 75.30
	 F1: 80.47
--------------------slot--------------------
	 Precision: 82.60
	 Recall: 78.43
	 F1: 80.46
--------------------overall--------------------
	 Precision: 83.98
	 Recall: 77.24
	 F1: 80.47
best val F1 0.8047
save on crosswoz/output/all
[3000|40000] step
	 slot loss: 0.13690621549030765
	 intent loss: 0.011509994967083912
8458 samples val
	 slot loss: 0.13520064545450894
	 intent loss: 0.008825290162534933
--------------------intent--------------------
	 Precision: 87.15
	 Recall: 84.11
	 F1: 85.60
--------------------slot--------------------
	 Precision: 84.44
	 Recall: 80.94
	 F1: 82.66
--------------------overall--------------------
	 Precision: 85.48
	 Recall: 82.16
	 F1: 83.79
best val F1 0.8379
save on crosswoz/output/all
[4000|40000] step
	 slot loss: 0.1261549115728121
	 intent loss: 0.009358842187852132
8458 samples val
	 slot loss: 0.12483063474396404
	 intent loss: 0.007739925438195306
--------------------intent--------------------
	 Precision: 86.86
	 Recall: 86.35
	 F1: 86.61
--------------------slot--------------------
	 Precision: 85.64
	 Recall: 81.64
	 F1: 83.59
--------------------overall--------------------
	 Precision: 86.12
	 Recall: 83.44
	 F1: 84.76
best val F1 0.8476
save on crosswoz/output/all
[5000|40000] step
	 slot loss: 0.11408365613769274
	 intent loss: 0.007924058733915445
8458 samples val
	 slot loss: 0.11575665574631598
	 intent loss: 0.006834385048114221
--------------------intent--------------------
	 Precision: 87.65
	 Recall: 87.60
	 F1: 87.62
--------------------slot--------------------
	 Precision: 87.78
	 Recall: 83.31
	 F1: 85.49
--------------------overall--------------------
	 Precision: 87.73
	 Recall: 84.95
	 F1: 86.32
best val F1 0.8632
save on crosswoz/output/all
[6000|40000] step
	 slot loss: 0.10137246547755785
	 intent loss: 0.007465936991095077
8458 samples val
	 slot loss: 0.12095139262790147
	 intent loss: 0.006434283603277748
--------------------intent--------------------
	 Precision: 87.56
	 Recall: 88.37
	 F1: 87.96
--------------------slot--------------------
	 Precision: 88.08
	 Recall: 84.20
	 F1: 86.10
--------------------overall--------------------
	 Precision: 87.88
	 Recall: 85.79
	 F1: 86.82
best val F1 0.8682
save on crosswoz/output/all
[7000|40000] step
	 slot loss: 0.10103354448394385
	 intent loss: 0.006704559793026419
8458 samples val
	 slot loss: 0.11396219044847586
	 intent loss: 0.005888600296032956
--------------------intent--------------------
	 Precision: 88.33
	 Recall: 89.59
	 F1: 88.96
--------------------slot--------------------
	 Precision: 87.45
	 Recall: 83.83
	 F1: 85.60
--------------------overall--------------------
	 Precision: 87.80
	 Recall: 86.03
	 F1: 86.91
best val F1 0.8691
save on crosswoz/output/all
[8000|40000] step
	 slot loss: 0.09368193136958872
	 intent loss: 0.006300244424215634
8458 samples val
	 slot loss: 0.11191419346116166
	 intent loss: 0.005572637868674889
--------------------intent--------------------
	 Precision: 88.13
	 Recall: 90.35
	 F1: 89.22
--------------------slot--------------------
	 Precision: 89.30
	 Recall: 85.52
	 F1: 87.37
--------------------overall--------------------
	 Precision: 88.83
	 Recall: 87.36
	 F1: 88.09
best val F1 0.8809
save on crosswoz/output/all
[9000|40000] step
	 slot loss: 0.0888186638612533
	 intent loss: 0.005488363280092017
8458 samples val
	 slot loss: 0.11011841898855834
	 intent loss: 0.0050762679199408455
--------------------intent--------------------
	 Precision: 88.11
	 Recall: 91.20
	 F1: 89.63
--------------------slot--------------------
	 Precision: 89.60
	 Recall: 85.42
	 F1: 87.46
--------------------overall--------------------
	 Precision: 89.00
	 Recall: 87.63
	 F1: 88.31
best val F1 0.8831
save on crosswoz/output/all
[10000|40000] step
	 slot loss: 0.086617979125469
	 intent loss: 0.005324330958974315
8458 samples val
	 slot loss: 0.10853510054295672
	 intent loss: 0.005024028395344133
--------------------intent--------------------
	 Precision: 88.85
	 Recall: 90.79
	 F1: 89.81
--------------------slot--------------------
	 Precision: 89.40
	 Recall: 85.28
	 F1: 87.29
--------------------overall--------------------
	 Precision: 89.18
	 Recall: 87.38
	 F1: 88.27
[11000|40000] step
	 slot loss: 0.07806921838806011
	 intent loss: 0.005082258003356401
8458 samples val
	 slot loss: 0.10203947061525258
	 intent loss: 0.004802110374196437
--------------------intent--------------------
	 Precision: 88.34
	 Recall: 91.56
	 F1: 89.92
--------------------slot--------------------
	 Precision: 90.40
	 Recall: 85.88
	 F1: 88.08
--------------------overall--------------------
	 Precision: 89.57
	 Recall: 88.05
	 F1: 88.80
best val F1 0.8880
save on crosswoz/output/all
[12000|40000] step
	 slot loss: 0.08151068553380901
	 intent loss: 0.0049187103233925885
8458 samples val
	 slot loss: 0.1039985217352144
	 intent loss: 0.004529812458653635
--------------------intent--------------------
	 Precision: 89.12
	 Recall: 92.08
	 F1: 90.58
--------------------slot--------------------
	 Precision: 90.31
	 Recall: 86.32
	 F1: 88.27
--------------------overall--------------------
	 Precision: 89.84
	 Recall: 88.52
	 F1: 89.17
best val F1 0.8917
save on crosswoz/output/all
[13000|40000] step
	 slot loss: 0.07667632893146947
	 intent loss: 0.004591984558959666
8458 samples val
	 slot loss: 0.10325655337063241
	 intent loss: 0.004490584614350002
--------------------intent--------------------
	 Precision: 88.39
	 Recall: 92.66
	 F1: 90.48
--------------------slot--------------------
	 Precision: 89.95
	 Recall: 86.14
	 F1: 88.00
--------------------overall--------------------
	 Precision: 89.32
	 Recall: 88.63
	 F1: 88.97
[14000|40000] step
	 slot loss: 0.07192586060019676
	 intent loss: 0.004491449447843479
8458 samples val
	 slot loss: 0.10483045259323626
	 intent loss: 0.004339679343125639
--------------------intent--------------------
	 Precision: 88.72
	 Recall: 93.00
	 F1: 90.81
--------------------slot--------------------
	 Precision: 90.50
	 Recall: 85.86
	 F1: 88.12
--------------------overall--------------------
	 Precision: 89.78
	 Recall: 88.59
	 F1: 89.18
best val F1 0.8918
save on crosswoz/output/all
[15000|40000] step
	 slot loss: 0.06751302678283537
	 intent loss: 0.004337910047732293
8458 samples val
	 slot loss: 0.10688661386160185
	 intent loss: 0.004279895916919444
--------------------intent--------------------
	 Precision: 88.85
	 Recall: 92.94
	 F1: 90.85
--------------------slot--------------------
	 Precision: 90.46
	 Recall: 86.14
	 F1: 88.25
--------------------overall--------------------
	 Precision: 89.81
	 Recall: 88.74
	 F1: 89.27
best val F1 0.8927
save on crosswoz/output/all
[16000|40000] step
	 slot loss: 0.06738211297642556
	 intent loss: 0.004054399402768467
8458 samples val
	 slot loss: 0.10223516412711925
	 intent loss: 0.004045101566964505
--------------------intent--------------------
	 Precision: 88.97
	 Recall: 93.55
	 F1: 91.20
--------------------slot--------------------
	 Precision: 90.57
	 Recall: 86.36
	 F1: 88.41
--------------------overall--------------------
	 Precision: 89.92
	 Recall: 89.11
	 F1: 89.51
best val F1 0.8951
save on crosswoz/output/all
[17000|40000] step
	 slot loss: 0.06422315696289298
	 intent loss: 0.004102494679566007
8458 samples val
	 slot loss: 0.09697883366642916
	 intent loss: 0.003991564307835871
--------------------intent--------------------
	 Precision: 90.27
	 Recall: 92.95
	 F1: 91.59
--------------------slot--------------------
	 Precision: 91.16
	 Recall: 86.65
	 F1: 88.85
--------------------overall--------------------
	 Precision: 90.80
	 Recall: 89.06
	 F1: 89.92
best val F1 0.8992
save on crosswoz/output/all
[18000|40000] step
	 slot loss: 0.06368641651945654
	 intent loss: 0.003814661967859138
8458 samples val
	 slot loss: 0.09770361758504512
	 intent loss: 0.004035925141887753
--------------------intent--------------------
	 Precision: 89.28
	 Recall: 93.55
	 F1: 91.37
--------------------slot--------------------
	 Precision: 91.08
	 Recall: 86.55
	 F1: 88.75
--------------------overall--------------------
	 Precision: 90.35
	 Recall: 89.22
	 F1: 89.78
[19000|40000] step
	 slot loss: 0.06219236230978276
	 intent loss: 0.0037052671232886496
8458 samples val
	 slot loss: 0.10064653516361667
	 intent loss: 0.003930714159636336
--------------------intent--------------------
	 Precision: 89.08
	 Recall: 93.69
	 F1: 91.33
--------------------slot--------------------
	 Precision: 91.53
	 Recall: 87.08
	 F1: 89.25
--------------------overall--------------------
	 Precision: 90.54
	 Recall: 89.61
	 F1: 90.07
best val F1 0.9007
save on crosswoz/output/all
[20000|40000] step
	 slot loss: 0.053251878017676064
	 intent loss: 0.0036967035689158367
8458 samples val
	 slot loss: 0.10578116499899794
	 intent loss: 0.0038332790733175705
--------------------intent--------------------
	 Precision: 89.35
	 Recall: 93.89
	 F1: 91.57
--------------------slot--------------------
	 Precision: 91.09
	 Recall: 86.64
	 F1: 88.81
--------------------overall--------------------
	 Precision: 90.39
	 Recall: 89.41
	 F1: 89.90
[21000|40000] step
	 slot loss: 0.05376750592765166
	 intent loss: 0.0036330682032275946
8458 samples val
	 slot loss: 0.09659909109479996
	 intent loss: 0.0038092930763902215
--------------------intent--------------------
	 Precision: 88.54
	 Recall: 94.64
	 F1: 91.49
--------------------slot--------------------
	 Precision: 91.61
	 Recall: 87.14
	 F1: 89.32
--------------------overall--------------------
	 Precision: 90.35
	 Recall: 90.00
	 F1: 90.18
best val F1 0.9018
save on crosswoz/output/all
[22000|40000] step
	 slot loss: 0.05034797015460208
	 intent loss: 0.0034685801764353526
8458 samples val
	 slot loss: 0.09499155242018567
	 intent loss: 0.0037781985639756908
--------------------intent--------------------
	 Precision: 89.20
	 Recall: 94.31
	 F1: 91.68
--------------------slot--------------------
	 Precision: 91.51
	 Recall: 87.08
	 F1: 89.24
--------------------overall--------------------
	 Precision: 90.57
	 Recall: 89.84
	 F1: 90.20
best val F1 0.9020
save on crosswoz/output/all
[23000|40000] step
	 slot loss: 0.047409598297439515
	 intent loss: 0.0033932143468482535
8458 samples val
	 slot loss: 0.1072073005929313
	 intent loss: 0.0036972378902218623
--------------------intent--------------------
	 Precision: 89.10
	 Recall: 94.47
	 F1: 91.71
--------------------slot--------------------
	 Precision: 90.83
	 Recall: 86.68
	 F1: 88.71
--------------------overall--------------------
	 Precision: 90.13
	 Recall: 89.66
	 F1: 89.89
[24000|40000] step
	 slot loss: 0.047846370869432574
	 intent loss: 0.003321696885403071
8458 samples val
	 slot loss: 0.1002099511179356
	 intent loss: 0.0036858047373360603
--------------------intent--------------------
	 Precision: 89.07
	 Recall: 94.52
	 F1: 91.71
--------------------slot--------------------
	 Precision: 91.26
	 Recall: 86.93
	 F1: 89.04
--------------------overall--------------------
	 Precision: 90.36
	 Recall: 89.83
	 F1: 90.09
[25000|40000] step
	 slot loss: 0.04723271680405014
	 intent loss: 0.003098164265771629
8458 samples val
	 slot loss: 0.10303303738821316
	 intent loss: 0.0036501473536196204
--------------------intent--------------------
	 Precision: 89.48
	 Recall: 94.43
	 F1: 91.89
--------------------slot--------------------
	 Precision: 91.83
	 Recall: 87.25
	 F1: 89.48
--------------------overall--------------------
	 Precision: 90.88
	 Recall: 89.99
	 F1: 90.43
best val F1 0.9043
save on crosswoz/output/all
[26000|40000] step
	 slot loss: 0.044770474568533246
	 intent loss: 0.003161883782628138
8458 samples val
	 slot loss: 0.1014854489288267
	 intent loss: 0.003593406487566266
--------------------intent--------------------
	 Precision: 90.15
	 Recall: 93.80
	 F1: 91.94
--------------------slot--------------------
	 Precision: 91.94
	 Recall: 87.39
	 F1: 89.61
--------------------overall--------------------
	 Precision: 91.21
	 Recall: 89.84
	 F1: 90.52
best val F1 0.9052
save on crosswoz/output/all
[27000|40000] step
	 slot loss: 0.042486833746545015
	 intent loss: 0.0033012829672370574
8458 samples val
	 slot loss: 0.10628608298342009
	 intent loss: 0.00355639921516254
--------------------intent--------------------
	 Precision: 89.33
	 Recall: 94.20
	 F1: 91.70
--------------------slot--------------------
	 Precision: 91.62
	 Recall: 86.96
	 F1: 89.23
--------------------overall--------------------
	 Precision: 90.69
	 Recall: 89.72
	 F1: 90.20
[28000|40000] step
	 slot loss: 0.03975314407973201
	 intent loss: 0.0029934964751555525
8458 samples val
	 slot loss: 0.10947188607873913
	 intent loss: 0.003499723504000327
--------------------intent--------------------
	 Precision: 90.19
	 Recall: 94.09
	 F1: 92.10
--------------------slot--------------------
	 Precision: 91.71
	 Recall: 87.11
	 F1: 89.35
--------------------overall--------------------
	 Precision: 91.09
	 Recall: 89.78
	 F1: 90.43
[29000|40000] step
	 slot loss: 0.03954962515999796
	 intent loss: 0.0030590458039878284
8458 samples val
	 slot loss: 0.10132512197439493
	 intent loss: 0.003514212578262403
--------------------intent--------------------
	 Precision: 89.75
	 Recall: 94.06
	 F1: 91.85
--------------------slot--------------------
	 Precision: 92.00
	 Recall: 87.55
	 F1: 89.72
--------------------overall--------------------
	 Precision: 91.09
	 Recall: 90.04
	 F1: 90.56
best val F1 0.9056
save on crosswoz/output/all
[30000|40000] step
	 slot loss: 0.03754820245460724
	 intent loss: 0.003105858320304833
8458 samples val
	 slot loss: 0.10292354668394929
	 intent loss: 0.00346290593965073
--------------------intent--------------------
	 Precision: 90.57
	 Recall: 93.86
	 F1: 92.18
--------------------slot--------------------
	 Precision: 91.82
	 Recall: 87.45
	 F1: 89.58
--------------------overall--------------------
	 Precision: 91.32
	 Recall: 89.90
	 F1: 90.60
best val F1 0.9060
save on crosswoz/output/all
[31000|40000] step
	 slot loss: 0.03504598879866535
	 intent loss: 0.0028106887066023773
8458 samples val
	 slot loss: 0.10289625687649845
	 intent loss: 0.0035058569180161626
--------------------intent--------------------
	 Precision: 89.72
	 Recall: 94.21
	 F1: 91.91
--------------------slot--------------------
	 Precision: 92.00
	 Recall: 87.33
	 F1: 89.60
--------------------overall--------------------
	 Precision: 91.07
	 Recall: 89.96
	 F1: 90.51
[32000|40000] step
	 slot loss: 0.03410125330204028
	 intent loss: 0.0028069130507174123
8458 samples val
	 slot loss: 0.10668325835820154
	 intent loss: 0.0034481005295902514
--------------------intent--------------------
	 Precision: 89.80
	 Recall: 94.63
	 F1: 92.15
--------------------slot--------------------
	 Precision: 92.20
	 Recall: 87.65
	 F1: 89.87
--------------------overall--------------------
	 Precision: 91.22
	 Recall: 90.31
	 F1: 90.77
best val F1 0.9077
save on crosswoz/output/all
[33000|40000] step
	 slot loss: 0.029073962645605207
	 intent loss: 0.0026435327868639434
8458 samples val
	 slot loss: 0.11078876876190916
	 intent loss: 0.003508233453210868
--------------------intent--------------------
	 Precision: 89.49
	 Recall: 94.38
	 F1: 91.87
--------------------slot--------------------
	 Precision: 92.25
	 Recall: 87.71
	 F1: 89.92
--------------------overall--------------------
	 Precision: 91.13
	 Recall: 90.26
	 F1: 90.69
[34000|40000] step
	 slot loss: 0.033428493934858125
	 intent loss: 0.0028007223893146147
8458 samples val
	 slot loss: 0.1048438696020221
	 intent loss: 0.003403047772627199
--------------------intent--------------------
	 Precision: 89.86
	 Recall: 94.81
	 F1: 92.27
--------------------slot--------------------
	 Precision: 92.30
	 Recall: 87.85
	 F1: 90.02
--------------------overall--------------------
	 Precision: 91.31
	 Recall: 90.51
	 F1: 90.91
best val F1 0.9091
save on crosswoz/output/all
[35000|40000] step
	 slot loss: 0.028175065322197042
	 intent loss: 0.0028609173849908982
8458 samples val
	 slot loss: 0.10760436593653533
	 intent loss: 0.0034224862733618663
--------------------intent--------------------
	 Precision: 90.18
	 Recall: 94.49
	 F1: 92.29
--------------------slot--------------------
	 Precision: 92.29
	 Recall: 87.66
	 F1: 89.91
--------------------overall--------------------
	 Precision: 91.44
	 Recall: 90.27
	 F1: 90.85
[36000|40000] step
	 slot loss: 0.025834057907995884
	 intent loss: 0.0026842195706303757
8458 samples val
	 slot loss: 0.11249141213426712
	 intent loss: 0.003418967724050004
--------------------intent--------------------
	 Precision: 89.64
	 Recall: 94.66
	 F1: 92.08
--------------------slot--------------------
	 Precision: 92.34
	 Recall: 87.78
	 F1: 90.00
--------------------overall--------------------
	 Precision: 91.24
	 Recall: 90.41
	 F1: 90.82
[37000|40000] step
	 slot loss: 0.026764804848324275
	 intent loss: 0.0026514431207706364
8458 samples val
	 slot loss: 0.1153481000896141
	 intent loss: 0.003402178003264703
--------------------intent--------------------
	 Precision: 90.05
	 Recall: 94.83
	 F1: 92.37
--------------------slot--------------------
	 Precision: 92.40
	 Recall: 87.70
	 F1: 89.99
--------------------overall--------------------
	 Precision: 91.44
	 Recall: 90.42
	 F1: 90.93
best val F1 0.9093
save on crosswoz/output/all
[38000|40000] step
	 slot loss: 0.024567880613467423
	 intent loss: 0.002588034704556776
8458 samples val
	 slot loss: 0.11201981308114695
	 intent loss: 0.0034254415376195908
--------------------intent--------------------
	 Precision: 89.99
	 Recall: 94.50
	 F1: 92.19
--------------------slot--------------------
	 Precision: 92.52
	 Recall: 87.90
	 F1: 90.15
--------------------overall--------------------
	 Precision: 91.49
	 Recall: 90.42
	 F1: 90.95
best val F1 0.9095
save on crosswoz/output/all
[39000|40000] step
	 slot loss: 0.024297734581072292
	 intent loss: 0.0026298240429423457
8458 samples val
	 slot loss: 0.11191112635326637
	 intent loss: 0.003403610864157897
--------------------intent--------------------
	 Precision: 89.89
	 Recall: 94.58
	 F1: 92.18
--------------------slot--------------------
	 Precision: 92.32
	 Recall: 87.68
	 F1: 89.94
--------------------overall--------------------
	 Precision: 91.33
	 Recall: 90.31
	 F1: 90.82
[40000|40000] step
	 slot loss: 0.020815901559457416
	 intent loss: 0.0026389145754346826
8458 samples val
	 slot loss: 0.11171553421231739
	 intent loss: 0.0033966933464988735
--------------------intent--------------------
	 Precision: 89.94
	 Recall: 94.69
	 F1: 92.25
--------------------slot--------------------
	 Precision: 92.34
	 Recall: 87.73
	 F1: 89.97
--------------------overall--------------------
	 Precision: 91.37
	 Recall: 90.39
	 F1: 90.87
zip model to crosswoz/output/all/bert_crosswoz_all.zip
